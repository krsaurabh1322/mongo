
import java.util.ArrayList;
import java.util.List;

public class JoinConfig {

    public enum JoinType {
        INNER, LEFT, RIGHT, FULL
    }

    public static class JoinCondition {
        private String leftColumn;
        private String rightColumn;
        private String filter;

        public JoinCondition(String leftColumn, String rightColumn, String filter) {
            this.leftColumn = leftColumn;
            this.rightColumn = rightColumn;
            this.filter = filter;
        }

        public String getLeftColumn() {
            return leftColumn;
        }

        public String getRightColumn() {
            return rightColumn;
        }

        public String getFilter() {
            return filter;
        }
    }

    private JoinType joinType;
    private List<JoinCondition> joinConditions;

    public JoinConfig(JoinType joinType) {
        this.joinType = joinType;
        this.joinConditions = new ArrayList<>();
    }

    public void addJoinCondition(String leftColumn, String rightColumn, String filter) {
        JoinCondition condition = new JoinCondition(leftColumn, rightColumn, filter);
        joinConditions.add(condition);
    }

    public JoinType getJoinType() {
        return joinType;
    }

    public List<JoinCondition> getJoinConditions() {
        return joinConditions;
    }
}


import java.util.List;

public class SparkMongoDataSource implements SparkDataSource<List<Document>> {

    // Existing methods and constructors
    
    // New join method
    public Dataset<Row> join(SparkSession sparkSession, Dataset<Row> leftDataset, String leftAlias,
                             Dataset<Row> rightDataset, String rightAlias, JoinConfig joinConfig) {
        // Build join conditions
        Column[] joinConditions = joinConfig.getJoinConditions().stream()
                .map(condition -> functions.expr(condition.getFilter()))
                .toArray(Column[]::new);
        
        // Perform join
        return leftDataset.alias(leftAlias)
                .join(rightDataset.alias(rightAlias), joinConditions, joinConfig.getJoinType().toString());
    }
}


v
public class SparkMongoDataSource implements SparkDataSource<List<Document>> {

    // Existing methods and constructors
    
    // New join method
    public Dataset<Row> join(SparkSession sparkSession, Dataset<Row> leftDataset, String leftAlias,
                             Dataset<Row> rightDataset, String rightAlias, JoinConfig joinConfig) {
        // Build join conditions
        Column[] joinConditions = joinConfig.getJoinConditions().stream()
                .map(condition -> functions.expr(condition.getFilter()))
                .toArray(Column[]::new);
        
        // Perform join
        return leftDataset.alias(leftAlias)
                .join(rightDataset.alias(rightAlias), joinConditions, joinConfig.getJoinType().toString());
    }
}

/////////



import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class JoinConfig {
    public enum JoinType {
        INNER, LEFT, RIGHT, FULL
    }

    private JoinType joinType;
    private List<JoinCondition> joinConditions;
    private List<String> groupByColumns;
    private List<String> selectColumns;
    private Map<String, String> aggregateFunctions;

    public JoinConfig(JoinType joinType) {
        this.joinType = joinType;
        this.joinConditions = new ArrayList<>();
        this.groupByColumns = new ArrayList<>();
        this.selectColumns = new ArrayList<>();
        this.aggregateFunctions = new HashMap<>();
    }

    public void addJoinCondition(String leftColumn, String rightColumn, String filter) {
        joinConditions.add(new JoinCondition(leftColumn, rightColumn, filter));
    }

    public void addGroupByColumn(String columnName) {
        groupByColumns.add(columnName);
    }

    public void addSelectColumn(String columnName) {
        selectColumns.add(columnName);
    }

    public void addAggregateFunction(String columnName, String aggregateFunction) {
        aggregateFunctions.put(columnName, aggregateFunction);
    }

    public JoinType getJoinType() {
        return joinType;
    }

    public List<JoinCondition> getJoinConditions() {
        return joinConditions;
    }

    public List<String> getGroupByColumns() {
        return groupByColumns;
    }

    public List<String> getSelectColumns() {
        return selectColumns;
    }

    public Map<String, String> getAggregateFunctions() {
        return aggregateFunctions;
    }

    public static class JoinCondition {
        private String leftColumn;
        private String rightColumn;
        private String filter;

        public JoinCondition(String leftColumn, String rightColumn, String filter) {
            this.leftColumn = leftColumn;
            this.rightColumn = rightColumn;
            this.filter = filter;
        }

        public String getLeftColumn() {
            return leftColumn;
        }

        public String getRightColumn() {
            return rightColumn;
        }

        public String getFilter() {
            return filter;
        }
    }
}




public class SparkMongoDataSourceTest {

    public static void main(String[] args) {
        // Existing code
        
        // Define join conditions, group by columns, and aggregate functions
        JoinConfig joinConfigXYZ = new JoinConfig(JoinConfig.JoinType.LEFT);
        joinConfigXYZ.addJoinCondition("X.id", "Y.id", null);
        joinConfigXYZ.addJoinCondition("X.id", "Z.id", null);
        joinConfigXYZ.addGroupByColumn("X.id");
        joinConfigXYZ.addAggregateFunction("Y.total", "sum(Y.total)");
        joinConfigXYZ.addAggregateFunction("Z.value", "avg(Z.value)");
        joinConfigXYZ.addSelectColumn("X.id");
        joinConfigXYZ.addSelectColumn("Y.name");
        joinConfigXYZ.addSelectColumn("Z.value");

        // Perform join with group by and aggregation
        Dataset<Row> joinedXYZ = dataSource.join(spark, datasetX, "X", datasetY, "Y", joinConfigXYZ);
        joinedXYZ.show();

        spark.stop();
    }
}



import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;

import java.util.List;
import java.util.Map;

public class SparkMongoDataSource implements SparkDataSource<List<Document>> {
    // Existing methods
    
    public Dataset<Row> join(SparkSession sparkSession, Dataset<Row> leftDataset, String leftAlias,
                             Dataset<Row> rightDataset, String rightAlias, JoinConfig joinConfig) {
        // Build join conditions
        Column[] joinConditions = joinConfig.getJoinConditions().stream()
                .map(condition -> functions.expr(condition.getFilter()))
                .toArray(Column[]::new);
        
        // Build group by columns
        Column[] groupByColumns = joinConfig.getGroupByColumns().stream()
                .map(colName -> functions.col(colName))
                .toArray(Column[]::new);

        // Build select columns
        Column[] selectColumns = joinConfig.getSelectColumns().stream()
                .map(colName -> functions.col(colName))
                .toArray(Column[]::new);
        
        // Build aggregate functions
        Column[] aggregateColumns = joinConfig.getAggregateFunctions().entrySet().stream()
                .map(entry -> functions.expr(entry.getValue()).as(entry.getKey()))
                .toArray(Column[]::new);
        
        // Perform join and group by
        Dataset<Row> joinedData = leftDataset.alias(leftAlias)
                .join(rightDataset.alias(rightAlias), joinConditions, joinConfig.getJoinType().toString())
                .groupBy(groupByColumns)
                .agg(aggregateColumns[0], aggregateColumns); // Applying multiple aggregate functions
        
        // Select the final columns
        return joinedData.select(selectColumns);
    }
}
