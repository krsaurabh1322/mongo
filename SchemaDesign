jiSchema design for versioning and historical records in MongoDB

A single collection for both the main data and the historical records.
By using a separate version field and a history array, we can track the document's changes over time 
while keeping the _id field consistent.

db.counterparties.insertOne({
  _id: 1,
  counterparty_id: 1,
  version: 2,
  name: "Counterparty 1",
  address: "Address 1 New",
  startdate: ISODate("2023-07-10T09:30:00Z"),
  history: [
    {
      counterparty_id: 1,
      version: 1,
      name: "Counterparty 1",
      address: "Address 1",
      startdate: ISODate("2023-07-03T10:30:00Z"),
      closedate: ISODate("2023-07-10T09:30:00Z")      
    },
    // Other historical versions
  ]
})

OR
We can keep the main data collection separate from the historical records collection, thus
providing better organization and potentially optimizing performance.

db.counterparties.insertOne({
  _id: 1, // Unique identifier for the counterparty
  counterparty_id: 1,
  name: "Counterparty 1",
  address: "Address 1 New",
  startdate: ISODate("2023-07-10T09:30:00Z"),
  version: 2 // Current version number of the counterparty record
})

db.counterparties_history.insertOne({
  _id: ObjectId(), // a new unique identifier for each historical record or use counterparty_id and version as composite primary key
  counterparty_id: 1,
  version: 1,
  name: "Counterparty 1",
  address: "Address 1",
  startdate: ISODate("2023-07-03T10:30:00Z"),
  closedate: ISODate("2023-07-10T09:30:00Z")  
})

Schema for both the main table (counterparties) and the historical table (counterparties_history)

CREATE TABLE counterparties (
  counterparty_id VARCHAR(100) PRIMARY KEY,
  name VARCHAR(100),
  address VARCHAR(1000),
  startdate TIMESTAMP,
  closedate TIMESTAMP
);

CREATE TABLE counterparties_history (
  id VARCHAR(100) PRIMARY KEY, // counterparty_id and version together form the composite primary key
  counterparty_id VARCHAR(100),
  version INTEGER,
  name VARCHAR(100),
  address VARCHAR(1000),
  startdate TIMESTAMP,
  closedate TIMESTAMP,
  FOREIGN KEY (counterparty_id) REFERENCES counterparties (counterparty_id)
);

An implemenatin of update method for Mongodb:

Retrieves the document from Hazelcast, if the document is not found in Hazelcast, it falls back to retrieving it from MongoDB.
Clones the existing document to create a historical record.
Gets the current version and increments it.
Updates the version and data fields in the document with the new values.
Creates a new historical version with the previous version number, current timestamp, and the data from the cloned document.
Adds the historical version to the history array of the cloned document.
Replaces the document in MongoDB with the updated version.
Updates the document in Hazelcast.
Inserts the historical record into the MongoDB collection.

import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.IMap;
import com.mongodb.client.MongoCollection;
import com.mongodb.client.MongoDatabase;
import org.bson.Document;

import java.util.Date;

public class MongoDBVersioningExample {
    private final HazelcastInstance hazelcastInstance;
    private final MongoDatabase mongoDatabase;
    private final String collectionName;

    public MongoDBVersioningExample(HazelcastInstance hazelcastInstance, MongoDatabase mongoDatabase, String collectionName) {
        this.hazelcastInstance = hazelcastInstance;
        this.mongoDatabase = mongoDatabase;
        this.collectionName = collectionName;
    }

    public void updateDocument(String documentId, Document updatedData) {
        // Get the distributed map from Hazelcast
        IMap<String, Document> documentMap = hazelcastInstance.getMap(collectionName);

        // Get the Mongo collection
        MongoCollection<Document> collection = mongoDatabase.getCollection(collectionName);

        // Retrieve the document from Hazelcast or MongoDB
        Document document = documentMap.get(documentId);
        if (document == null) {
            document = collection.find(new Document("_id", documentId)).first();
        }

        // Check if the document exists
        if (document != null) {
            // Clone the existing document to create a historical record
            Document historicalRecord = new Document(document);

            // Get the current version and increment it
            int currentVersion = document.getInteger("version");
            int newVersion = currentVersion + 1;

            // Update the version field in the document
            document.put("version", newVersion);

            // Set the updated data in the document
            document.put("data", updatedData);

            // Create a new historical record with the updated version and timestamp
            Document historicalVersion = new Document("version", currentVersion)
                    .append("timestamp", new Date())
                    .append("data", historicalRecord.get("data"));

            // Add the historical version to the history array
            ((List<Document>) historicalRecord.get("history")).add(historicalVersion);

            // Replace the document in MongoDB
            collection.replaceOne(new Document("_id", documentId), document);

            // Update the document in Hazelcast
            documentMap.put(documentId, document);

            // Insert the historical record into MongoDB
            collection.insertOne(historicalRecord);
        }
    }
}


Presentation outline:

Solution: Implement versioning in Database to track data changes and thus retain the full history of values in Database.

Versioning allows us to track and manage the different iterations or versions of a document/record over time. It also helps to 
retain the full history of values which is both auditable and efficient in terms of storage.

Implementaion: 
Design the schema
In order to track the historical changes of a document/record, we introduce a column in the collection/table to store the version information.
This information is updated/incremented with each update. 
As this scenario falls under a data warehouse concept called Slowly Changing Dimension (SCD), we apply the Type 2 SCD solution here. 
When the value of an attribute changes, the current record is closed and a new record is created with the changed data values and this 
new record becomes the current record. Each record contains the startdate time and closedate time to identify the time period between which the record was active. 


output = "NULL^NULL^NULL^NULL"

# Remove "NULL" at the start and end
if output.startswith("NULL^"):
    output = output[len("NULL^"):]
if output.endswith("^NULL"):
    output = output[:-len("^NULL")]

print(output)



//////////////

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.JsonProcessingException;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;

import static org.junit.Assert.*;
import static org.mockito.Mockito.*;

public class ObjectMapperUtilsTest {

    @Mock
    private ObjectMapperProvider objectMapperProvider;

    @Mock
    private Logger logger;

    @Before
    public void setUp() {
        MockitoAnnotations.initMocks(this);
    }

    @Test
    public void convertToObject_validJson_returnObject() {
        String jsonData = "{\"name\":\"John\",\"age\":30}";
        TypeReference<User> valueTypeRef = new TypeReference<User>() {};
        ObjectMapper objectMapper = new ObjectMapper();

        // Configure ObjectMapperProvider to return a mock ObjectMapper
        when(objectMapperProvider.get()).thenReturn(objectMapper);

        User user = new User("John", 30);

        try {
            String userJson = objectMapper.writeValueAsString(user);

            // Mock ObjectMapper readValue to return the user object
            when(objectMapper.readValue(jsonData, valueTypeRef)).thenReturn(user);

            User result = ObjectMapperUtils.convertToObject(jsonData, valueTypeRef, objectMapperProvider, logger);

            assertEquals(user, result);
        } catch (JsonProcessingException e) {
            fail("Unexpected JsonProcessingException");
        }

        verify(logger, never()).warn(anyString(), anyString(), any(JsonProcessingException.class));
    }

    @Test
    public void convertToObject_invalidJson_returnNull() {
        String jsonData = "InvalidJson";
        TypeReference<User> valueTypeRef = new TypeReference<User>() {};
        ObjectMapper objectMapper = new ObjectMapper();

        // Configure ObjectMapperProvider to return a mock ObjectMapper
        when(objectMapperProvider.get()).thenReturn(objectMapper);

        try {
            // Mock ObjectMapper readValue to throw a JsonProcessingException (simulate invalid JSON)
            when(objectMapper.readValue(jsonData, valueTypeRef)).thenThrow(JsonProcessingException.class);

            User result = ObjectMapperUtils.convertToObject(jsonData, valueTypeRef, objectMapperProvider, logger);

            assertNull(result);
        } catch (JsonProcessingException e) {
            fail("Unexpected JsonProcessingException");
        }

        verify(logger).warn(anyString(), anyString(), any(JsonProcessingException.class));
    }
}


// Custom temporal adjuster to set time fields to zero
        TemporalAdjuster timeZeroAdjuster = temporal -> {
            LocalDateTime dateTime = LocalDateTime.from(temporal);
            dateTime = dateTime.withHour(0).withMinute(0).withSecond(0).withNano(0);
            return dateTime;
        };


LocalDate date = LocalDate.parse(dateString, formatter.withChronology((localDateTime) -> {
            LocalDateTime adjustedDateTime = localDateTime.with(timeZeroAdjuster);
            return adjustedDateTime;
        }));



// Mock the behavior of ObjectMapperUtils
        when(ObjectMapperUtils.convertToObject(
                anyString(),
                any(TypeReference.class),
                any(ObjectMapperProvider.class),
                any(Logger.class)
        )).thenReturn(new BPSIRSRMASchema()); // Mock a valid BPSIRSRMASchema



////////


---

## Application Architecture Overview

Our application architecture is built around a repository interface that abstracts interactions with various data sources. The application utilizes Hazelcast for distributed caching, enhancing read performance and maintaining data consistency. The cache employs the Entity Follower pattern to synchronize with the underlying MongoDB database.

### Key Components:

1. **Repository Interface:**
   - Abstracts interactions with various data sources, such as MongoDB, file systems, or in-memory data.

2. **Hazelcast Cache:**
   - Serves as a distributed cache to improve read performance.
   - Implements the Entity Follower pattern to stay synchronized with updates from the MongoDB database.

3. **MongoDB Implementation:**
   - Provides the underlying data source implementation for MongoDB interactions.
   - Uses the Hazelcast cache to ensure data consistency.

---

## Entity Follower Pattern with Hazelcast

The Entity Follower pattern ensures that the cache (Hazelcast) closely mirrors the data stored in the MongoDB database. Here's how this is implemented:

1. **Cache Initialization:**
   - Upon application startup, Hazelcast initializes a cache to store data.
  
2. **Load Data from MongoDB:**
   - The cache is initially populated with data fetched from the MongoDB database using the repository interface.

3. **Entity Follower Setup:**
   - Hazelcast utilizes the Entity Follower pattern to observe updates in the MongoDB database.
   - Whenever the MongoDB database is updated, an event triggers a cache update in Hazelcast to keep the cache in sync.

4. **Synchronization Process:**
   - Updates in the MongoDB database (inserts, updates, deletes) trigger appropriate events.
   - These events are captured, and the corresponding data in Hazelcast is updated accordingly to reflect the changes.

5. **Read Operations:**
   - Whenever a read operation is requested, Hazelcast is queried first.
   - If the data is present in the cache, it's returned, improving read performance.
   - If not, the repository interface fetches the data from the MongoDB database, updates the cache, and then returns the data.

---

## Usage of Repository Interface

1. **Connecting to Data Source:**
   - The repository interface provides a unified way to connect to different data sources based on the application's needs.

2. **Abstracting Data Access:**
   - Through the repository interface, the application abstracts the complexities of interacting with various data sources (MongoDB, file systems, in-memory data).

3. **Consistent Data Access:**
   - Regardless of the underlying data source, the repository interface ensures consistent data access and manipulation by using Hazelcast for caching and synchronization.

---

This architecture and implementation design allow our application to achieve high read performance, data consistency, and seamless integration with various data sources, enabling a flexible and efficient data access layer.