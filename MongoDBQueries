



mongoexport --db your_database --collection your_collection --type csv --fields field1,field2,field3 --out output_file.csv
awk 'BEGIN { FPAT = "([^,]*)|(\"[^\"]+\")" } { for (i = 1; i <= NF; i++) gsub(",", "^", $i) } 1' OFS=, output_file.csv > temp_file.csv && mv temp_file.csv output_file.csv

Certainly! Let's break down the `awk` command step by step:

1. `BEGIN { FPAT = "([^,]*)|(\"[^\"]+\")" }`: This part sets the `FPAT` variable, which is a built-in `awk` variable used to define the field separator pattern. 
The pattern here is a regular expression that matches either non-comma characters `[^,]*` or strings enclosed in double quotes `\"[^\"]+\"`. 
This allows `awk` to consider quoted fields as a single field, even if they contain commas.

2. `{ for (i = 1; i <= NF; i++) gsub(",", "^", $i) }`: For each line processed by `awk`, this part loops through all fields (number of fields represented by `NF`) 
and replaces any occurrences of commas (,) with caret (^) in each field. 
The `gsub` function is used for global substitution, ensuring all commas within each field are replaced.

3. `1`: This is a common `awk` shorthand. It evaluates to true, and since no action is specified, it performs the default action, which is to print the entire line.

4. `OFS=,`: This sets the output field separator (OFS) to a comma (,). This ensures that `awk` writes the output back to CSV format, using commas as separators.

5. `> temp_file.csv && mv temp_file.csv output_file.csv`: This part of the command redirects the modified output to a temporary file (`temp_file.csv`). 
After successfully processing the file, it renames the temporary file to the original output file (`output_file.csv`).

Overall, this `awk` command uses a custom field separator pattern to handle quoted fields with commas correctly. It replaces commas with carets (^) in all fields 
while preserving the original CSV format.
Remember to back up your data before running any commands that modify files, and test the command on a sample CSV file to ensure it works as expected for your 
specific use case.




import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

public class MockDatasets {

    public static Dataset<Row> createFacilityDataset(SparkSession spark) {
        StructType schema = new StructType(new StructField[]{
                new StructField("FacilityID", DataTypes.IntegerType, false, null),
                new StructField("FacilityName", DataTypes.StringType, true, null)
        });

        return spark.createDataFrame(
                spark.sparkContext().parallelize(
                        java.util.Arrays.asList(
                                new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(
                                        new Object[]{1, "Facility 1"}, schema),
                                new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(
                                        new Object[]{2, "Facility 2"}, schema)
                        )
                ), schema);
    }

    public static Dataset<Row> createLimitDataset(SparkSession spark) {
        StructType schema = new StructType(new StructField[]{
                new StructField("LimitID", DataTypes.IntegerType, false, null),
                new StructField("Amount", DataTypes.DoubleType, true, null)
        });

        return spark.createDataFrame(
                spark.sparkContext().parallelize(
                        java.util.Arrays.asList(
                                new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(
                                        new Object[]{1, 1000.0}, schema),
                                new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(
                                        new Object[]{2, 2000.0}, schema)
                        )
                ), schema);
    }

    public static Dataset<Row> createCounterpartyDataset(SparkSession spark) {
        StructType schema = new StructType(new StructField[]{
                new StructField("CounterpartyID", DataTypes.IntegerType, false, null),
                new StructField("CounterpartyName", DataTypes.StringType, true, null)
        });

        return spark.createDataFrame(
                spark.sparkContext().parallelize(
                        java.util.Arrays.asList(
                                new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(
                                        new Object[]{1, "Counterparty 1"}, schema),
                                new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(
                                        new Object[]{2, "Counterparty 2"}, schema)
                        )
                ), schema);
    }
}

import static org.junit.Assert.*;
import static org.mockito.Mockito.*;

import org.apache.spark.sql.*;
import org.apache.spark.sql.functions;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Mock;
import org.mockito.MockitoAnnotations;

public class SparkJoinProcessorTest {
    @Mock
    private SparkSession sparkSession;

    private SparkJoinProcessor sparkJoinProcessor;

    @Before
    public void setUp() {
        MockitoAnnotations.initMocks(this);
        sparkJoinProcessor = new SparkJoinProcessor(new LoggerFactory(), new SparkDataFrameCacheManager(sparkSession));
    }

    @Test
    public void testPerformJoin() {
        // Mock the input request, joinConfig, leftDataset, Facility dataset
        SparkJobRequest request = createMockRequest();
        JoinConfig joinConfig = createMockJoinConfig();
        Dataset<Row> leftDataset = createMockLeftDataset();
        Dataset<Row> facilityDataset = createMockFacilityDataset();

        when(request.getJoinConfig()).thenReturn(joinConfig);
        when(sparkSession.createDataFrame(any(), any())).thenReturn(leftDataset, facilityDataset);

        // Call the method
        Dataset<Row> result = sparkJoinProcessor.performJoin(request);

        // Add assertions based on your expected results
        assertNotNull(result);
        // Add more specific assertions for the result dataset
    }

    @Test
    public void testPerformUnion() {
        // Mock the input request, unionConfig, leftDataset, Facility dataset
        SparkJobRequest request = createMockRequest();
        UnionConfig unionConfig = createMockUnionConfig();
        Dataset<Row> leftDataset = createMockLeftDataset();
        Dataset<Row> facilityDataset = createMockFacilityDataset();

        when(request.getUnionConfig()).thenReturn(unionConfig);
        when(sparkSession.createDataFrame(any(), any())).thenReturn(leftDataset, facilityDataset);

        // Call the method
        Dataset<Row> result = sparkJoinProcessor.performUnion(request);

        // Add assertions based on your expected results
        assertNotNull(result);
        // Add more specific assertions for the result dataset
    }

    @Test
    public void testPerformRecursiveOperation() {
        // Implement test for performRecursiveOperation
    }

    @Test
    public void testPerformSingleJoin() {
        // Implement test for performSingleJoin
    }

    @Test
    public void testApplyTransformation() {
        // Implement test for applyTransformation
    }

    @Test
    public void testGetJoinExpression() {
        // Implement test for getJoinExpression
    }

    // Create mock objects or use your actual test data for these methods
    private SparkJobRequest createMockRequest() {
        // Create and return a mock request
    }

    private JoinConfig createMockJoinConfig() {
        // Create and return a mock JoinConfig
    }

    private Dataset<Row> createMockLeftDataset() {
        // Create and return a mock left dataset
    }

    private Dataset<Row> createMockFacilityDataset() {
        // Create and return a mock Facility dataset
    }

    private UnionConfig createMockUnionConfig() {
        // Create and return a mock UnionConfig
    }
}



<build>
    <plugins>
        <plugin>
            <groupId>org.jsonschema2pojo</groupId>
            <artifactId>jsonschema2pojo-maven-plugin</artifactId>
            <version>1.1.1</version> <!-- Use the latest version -->
            <configuration>
                <sourceDirectory>${project.basedir}/src/main/resources</sourceDirectory>
                <targetPackage>com.example.generated</targetPackage>
                <generateBuilders>true</generateBuilders>
                <generateToString>true</generateToString>
            </configuration>
            <executions>
                <execution>
                    <goals>
                        <goal>generate</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>


import java.util.Optional;

Optional<YourGeneratedClass> optionalObject = Optional.ofNullable(yourObject);

optionalObject.ifPresent(obj -> {
    String name = obj.getName();
    String age = obj.getAge();

    System.out.println("Name: " + name);
    System.out.println("Age: " + age);
});

// Handle the case where 'yourObject' is null
optionalObject.orElseGet(() -> {
    System.out.println("Object is null");
    return new YourGeneratedClass(); // Return a default object if needed
});



import org.junit.Test;
import java.util.HashMap;
import java.util.Map;

import static org.junit.Assert.assertEquals;

public class SparkJobRequestTest {

    @Test
    public void testClone() {
        // Create a sample SparkJobRequest
        SparkJobRequest originalRequest = SparkJobRequest.builder()
                .uniqueRequestId("originalRequestId")
                .inputDataSource(SparkDataSourceConfig.builder()
                        .dataSourceType("inputDataSourceType")
                        .dataSourceConfig(new HashMap<>())
                        .build())
                .outputDataSource(SparkDataSourceConfig.builder()
                        .dataSourceType("outputDataSourceType")
                        .dataSourceConfig(new HashMap<>())
                        .build())
                .build();

        // Clone the original SparkJobRequest with a new input collection name
        String inputCollectionName = "newCollectionName";
        SparkJobRequest clonedRequest = originalRequest.clone(inputCollectionName);

        // Verify that the cloned request is correctly created
        assertEquals(originalRequest.getUniqueRequestId() + "_" + inputCollectionName, clonedRequest.getUniqueRequestId());
        assertEquals(inputCollectionName, clonedRequest.getInputDataSource().getDataSourceConfig().get(DataSourceParamKey.INPUT_COLLECTION_NAME));
        assertEquals(originalRequest.getOutputDataSource().getDataSourceType(), clonedRequest.getOutputDataSource().getDataSourceType());
        assertEquals(originalRequest.getOutputDataSource().getDataSourceConfig(), clonedRequest.getOutputDataSource().getDataSourceConfig());
    }
}



Of course! Adjusting the feedback to a third-person perspective is entirely feasible. Here's the revised version:

---

Subject: Performance Feedback - Promotion to Senior Developer

Dear [Employee's Name],

This communication is to recognize and appreciate the outstanding contributions of [Employee's Name] within the team. [Employee's Name]'s accomplishments in resolving intricate Hive query bugs, analyzing and optimizing processes, and proactively elevating the Python code base have distinguished them as an invaluable asset to the team.

1. **Expertise in Hive Query Debugging:**
   [Employee's Name]'s proficiency in resolving complex bugs within Hive queries has been pivotal in maintaining seamless data operations. Their keen attention to detail and analytical approach significantly enhanced query performance and data accuracy.

2. **Process Analysis and Optimization:**
   [Employee's Name]'s involvement in analyzing Hive queries and identifying areas for improvement has led to streamlined processes and increased overall efficiency in data operations. Their insights have played a crucial role in advancing query optimization strategies.

3. **Python Code Base Elevation:**
   [Employee's Name]'s proactive efforts in elevating the Python code base demonstrate their commitment to the development ecosystem. Their enhancements have not only improved code quality but also accelerated development velocity, reflecting their dedication to excellence.

4. **Transitioning Expertise:**
   [Employee's Name]'s dedication to expanding their skill set to encompass data engineering and Java proficiency is both impressive and forward-thinking. Their successful transition showcases adaptability and an aspiration to broaden impact within the team.

[Employee's Name]'s accomplishments underline expertise, dedication, and determination to excel. Considering their consistent achievements and versatile skill set, a strong recommendation is made for [Employee's Name]'s promotion to the next band as a Senior Developer. Their capabilities and dedication align well with the expectations and responsibilities associated with this role.

Appreciation goes to [Employee's Name] for tireless dedication and remarkable contributions. Anticipation is high for discussions regarding the promotion and future career trajectory.

Best regards,

[Your Name]
[Your Position]
[Your Contact Information]

---

Feel free to adapt this version to suit your requirements and the nuances of your team's culture.



///////


const fs = require('fs');
const csv = require('csv-parser');
const MongoClient = require('mongodb').MongoClient;

// MongoDB connection URL
const url = 'mongodb://localhost:27017';
const dbName = 'your-database-name';
const collectionName = 'your-collection-name';

// Load the CSV file with mappings
const mappings = [];
fs.createReadStream('mapping.csv')
  .pipe(csv())
  .on('data', (row) => {
    mappings.push(row);
  })
  .on('end', () => {
    // Connect to the MongoDB database
    MongoClient.connect(url, { useNewUrlParser: true }, (err, client) => {
      if (err) throw err;

      const db = client.db(dbName);
      const collection = db.collection(collectionName);

      // Iterate through each mapping
      mappings.forEach((mapping) => {
        const dealId = mapping.dealId;
        const oldProductId = mapping.old_productId;
        const newProductId = mapping.new_productId;
        const newDocumentId = `${dealId}_${newProductId}`;

        // Find the document with the old productId and dealId
        const query = { dealId: dealId, productId: oldProductId };
        collection.findOne(query, (err, doc) => {
          if (err) throw err;

          if (doc) {
            // Create a new document with the updated productId and _id
            const newDoc = { _id: newDocumentId, dealId: dealId, productId: newProductId, ...doc };

            // Insert the new document with the updated _id
            collection.insertOne(newDoc, (err, result) => {
              if (err) throw err;
            });
          }
        });
      });

      client.close();
    });
  });



///////



// Load the CSV file with mappings
var mappings = [];
var fileContent = cat('mapping.csv'); // Read the CSV file

// Split the CSV content into rows
var rows = fileContent.split('\n');
for (var i = 1; i < rows.length; i++) { // Skip the header row
  var columns = rows[i].split(',');
  var dealId = columns[0].trim();
  var oldProductId = columns[1].trim();
  var newProductId = columns[2].trim();
  
  // Find the existing document with the old _id
  var existingDocument = db.yourCollectionName.findOne({ _id: dealId + '_' + oldProductId });

  if (existingDocument) {
    // Create a new document based on the mapping
    var newDocument = {
      _id: dealId + '_' + newProductId, // Set the new _id
      dealId: dealId,
      productId: newProductId
    };

    // Copy all fields from the existing document to the new document
    for (var key in existingDocument) {
      if (existingDocument.hasOwnProperty(key) && key !== '_id') {
        newDocument[key] = existingDocument[key];
      }
    }

    // Insert the new document into the collection
    db.yourCollectionName.insert(newDocument);
  }
}
