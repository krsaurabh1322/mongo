
public class SparkDataFrameCacheManager (
public static final String ID = "_id";
private final Logger logger;
private final SparkDataSourceProvider dataSourceProvider;
private final StorageLevel cacheStorageLevel;
private final Map<DfCacheSourceKey, DfCacheData> cacheStore new ConcurrentHashMap<>();
@Inject
public SparkDataFrameCacheManager (final LoggerFactory LoggerFactory, final Config config, final SparkDataSourceProvider dataSourceProvider) {
this.logger = loggerFactory.create (SparkDataFrameCacheManager.class);
this.dataSourceProvider dataSourceProvider;
this.cacheStorageLevel StorageLevel.fromString (SparkCoreConfigDefinitions.SPARK_CORE_CACHE_STORAGE_LEVEL.get(config));
}
I
public void prepareHistoricalCache Refresh (final SparkJobRequest request) {
if (request.getCacheDfRequest Param().isRefreshCache()) {
final Map<String, List<Document>> keyFilterMap getCacheKeyFilterMap ( paramindicator: "VOLUME", request.getCacheDfRequestParam());
initializeMissingCachedDfMapEntries (request, keyFilterMap);
}
}



public Dataset<Row> doIntraDayCacheRefresh (final SparkJobRequest request, final SparkJobResponse.SparkJobResponse Builder builder) {
final LocalDate currentDate= LocalDate.now();
if (request.getCacheDfRequest Param().getPeriodStart().is Equal (request.getCacheDfRequest Param().getPeriodEnd())
&& currentDate.isEqual (request.getCacheDfRequest Param ().getPeriodStart())) {
Logger.info("Going to perform IntraDay Cache refresh for request-[{}]", request.getUniqueRequestId());
final List<Document> sparkPipelineFilter = DfCacheSourceKey.from SourceName (getInputSourceName (request))
.getSparkPipelineFilter (request, paramIndicator: "VOLUME");
final Dataset<Row> dataset = dataSourceProvider.ingest (request, sparkPipelineFilter);
logger.info("Going to cache records in Intraday Spark DF cache for spark Request- {}", request.getUniqueRequestId());
final boolean isEmpty = dataset.isEmpty();
if (!isEmpty) {
if (getorInitCachedSource (request).getIntraDaycachedDf() != null) {
getOrInitCachedSource (request).getIntraDaycachedDf().unpersist();
}
getOrInitCachedSource (request).setIntraDaycachedDf (cache (dataset));
return getorInitCachedSource (request).getIntraDaycachedDf();
} else {
builder.requestState (SparkJobRequestState. REJECTED);
builder.detailMessage("No records found for Intraday cache refresh request-" + request.getUniqueRequestId());
Logger.info("No records found for Intraday cache refresh request-{}", request.getUniqueRequestId());
} else {
builder.requestState (SparkJobRequestState. REJECTED);
builder.detailMessage ("Not a valid Intraday cache refresh request-" + request.getUniqueRequestId());
Logger.error("Not a valid Intraday cache refresh request-{}", request.getUniqueRequestId());
}

return null;
}


public Dataset<Row> doDeltaCache Refresh (final SparkJobRequest request, final SparkJobResponse.SparkJobResponseBuilder builder)
LocalDate startDate = getorInitCachedSource (request).getLastCacheRefresh();
startDate= startDate.plusDays (1);
final LocalDate endDate = LocalDate.now().minusDays (1);
final CacheDfRequest Param cacheDfRequest Param= request.getCacheDfRequest Param();
cacheDfRequest Param.setPeriodStart (startDate);
cacheDfRequest Param.setPeriodEnd (endDate);
if (!startDate.isAfter (endDate)) {
final List<Document> sparkPipelineFilter=DfCacheSourceKey.fromSourceName (getInputSourceName (request))
.getSparkPipelineFilter (request, paramIndicator: "VOLUME");
cacheDfRequestParam.setCacheFilter (sparkPipelineFilter);
Logger.info("Going to perform Delta DF Cache refresh for request-[{}]", request.getUniqueRequestId());
final Dataset<Row> dataset = dataSourceProvider.ingest (request, sparkPipelineFilter);
final boolean isEmpty = dataset.isEmpty();
Logger.info("Going to cache records for Delta Spark DF cache Request- {}", request.getUniqueRequestId());
final String key = endDate.format (KEY_FORMAT);
//Update Monthly cached DF to include delta for one day to handle Month roll over scenario
getOrInitCachedSource (request).getCachedDfKeys ().add(key);
if (!isEmpty) {
final Dataset<Row> df = cache (getOrInitCached Source (request).getUnionAllcachedDf().union (dataset));
getOrInitCachedSource (request).setUnion AllcachedDf (df);



return getorInitCachedSource (request).getUnionAllcachedDf();
} else {
}
} else {
}
builder.requestState (SparkJobRequestState. REJECTED);
builder.detailMessage("No records found for Delta DF cache refresh request-" + request.getUniqueRequestid());
logger.info("No records found for Delta DF cache refresh request-{}. StartDate-(), EndDate-{}",
request.getUniqueRequestId(), startDate, endDate);
public Dataset<Row> getUnionALLHistoricalDfCache(final SparkJobRequest request) {
return getorInitCachedSource (request).getUnionAllcachedDf();
builder.requestState (SparkJobRequestState. REJECTED);
builder.detailMessage("Not a valid Delta DF cache cache refresh request-" + request.getUniqueRequestId());
Logger.info("Not a valid Delta DF cache refresh request-{}. StartDate-{}, EndDate-{}",
request.getUniqueRequestId(), startDate, endDate);
return null;
private Set<String> getHistoricalDfCachedKeys (final SparkJobRequest request) {
return getorInitCachedSource (request).getCachedDfKeys ();
amn:
public Dataset<Row> getUnionHistoricalIntradayDfCache (final SparkJobRequest request) {
final SimpleTimerUtil timerUtil = new SimpleTimerUtil(logger);
Dataset<Row> udf = getOrInitCachedSource (request).getUnionAllcachedDf();
if (getorInitCachedSource (request).getIntraDaycachedDf() != null) {
udf = getOrInitCachedSource (request).getUnionALLcachedDf().union (getorInitCachedSource (request).getIntraDaycachedDf());
}

return udf;
}



public static Map<String, List<Document>> getCacheKeyFilterMap (final String paramIndicator,
final CacheDfRequest Param requestParam) (
}
final Map<String, List<Document>> keyFilterMap = new HashMap<> ();
final LocalDate initialStart = requestParam.getPeriodStart().with (TemporalAdjusters.firstDayOfMonth());
LocalDate initialEnd = requestParam.getPeriodEnd ();
if (!initialEnd.isBefore (LocalDate.now())) {
initialEnd LocalDate.now().minusDays (1);
}
LocalDate tempStart = initialStart;
while (tempStart.isBefore(initialEnd)) {
final String key = tempStart.format(KEY_FORMAT);
LocalDate tempEnd = tempStart.with
if (!tempEnd.isBefore(initialEnd)) {
tempEnd = initialEnd;
}
(TemporalAdjusters.lastDayOfMonth());
}
final List<Document> filter= buildSparkPipelineFilterForGiven Period (paramIndicator, tempStart, tempEnd);
keyFilterMap.put(key, filter);
tempStart = tempEnd.with (TemporalAdjusters.firstDayOfNext Month());
return keyFilterMap;
private void initializeMissingCachedDfMapEntries (final SparkJobRequest req, final Map<String. List<Document>> keyFilterMap) {
final String inputRecordSource = getInputSourceName (req);
final DfCacheData cacheData = getOrInitCached Source (inputRecordSource);
final SimpleTimerUtil timerUtil= new SimpleTimerUtil(logger);
timerUtil.LogEvent( eventName: "MongoSpark-Load", req.getUniqueRequestId());


if (!cacheData.getCachedDfKeys ().containsAll (keyFilterMap.keySet()) II req.getCacheDfRequestParam().isForce RefreshCache()) {
//req.getCacheDfRequest Param().getCacheFilter() should be same as entry.getValue()
Logger.info("Spark Request-for input source-{}. Refreshing DF Cache for keys-() required for this request.
+ "Pipeline Filter request-{}",
}
}
req.getUniqueRequestId(), inputRecordSource, keyFilterMap.keySet(),
req.getCacheDfRequest Param().getCacheFilter());
}
populateDfCacheMap (req, keyFilterMap, cacheData);
private DfCacheData getOrInitCachedSource (final String inputRecordSource) {
//sourceKey can not be null as already validated in SparkBIFunction.validateRequest()
final DfCacheSourceKey sourceKey = DfCacheSourceKey.fromSourceName (inputRecordSource);
cacheStore.putIfAbsent (sourceKey, new DfCacheData());
return cacheStore.get (sourceKey);
Logger.info("Spark Request-{} for input source-{}. Cache DF keys required for this request - {}",
req.getUniqueRequestId(), inputRecordSource, keyFilterMap.keySet());
Logger.info("Spark Request-{} for input source-{}. Cached DF keys- {}",
req.getUniqueRequestId(), inputRecordSource, cacheData.getCachedDfKeys ().toArray());
private DfCacheData getOrInitCachedSource (final SparkJobRequest req) {
return getorInitCachedSource (getInputSourceName (req));
private void populateDfCacheMap (final SparkJobRequest req, final Map<String, List<Document>> keyFilterMap,
final DfCacheData cacheData) {
final SimpleTimerUtil timerUtil = new SimpleTimerUtil(logger);




final Dataset<Row> dataset = dataSourceProvider.ingest(req,
req.getCacheDfRequest Param().getCacheFilter());
}
cacheData.setUnionAllcachedDf (cacheDf);
Logger.info("Initialized final UnionAll-DF cache for spark Request-{}", req.getUniqueRequestId());
Logger.info("Going to cache records in Spark DF cache for spark Request-() and keys-()", req.getUniqueRequestId(), keyFilterMap.keySet());
final Dataset<Row> dfTypeCast = columns Transformation (dataset);
final boolean isEmpty= dataset.isEmpty();
final Dataset<Row> cacheDf = !isEmpty ? cache(dfTypeCast): dataset;
keyFilterMap.forEach((k, v) -> cacheData.getCachedDfKeys ().add(k));
timerUtil.LogEvent( eventName: "Spark-populateDfCacheMap", req.getUniqueRequestId());
private Dataset<Row> columns Transformation (final Dataset<Row> dataset) {
final List<Column> colsTypeCast = stream (dataset.columns())
.map(câ†’> ID.equals(c) ? functions.col (c).cast (DataTypes.StringType): functions.col (c))
.collect(tolist());
SpandobRequest.class
return dataset.select(colsTypeCast.toArray (new Column[] {}));
CacheDfRequestParar
public boolean isDfCacheValid (final SparkJobRequest req, final SparkJobResponse, SparkJobResponseBuilder responseBuilder) {
boolean isValid = true;
if (req.getCacheDfRequest Param() != null && !req.getCacheDfRequest Param().isRefreshCache()) {



final Set<String> cachedKeys = getHistoricalDfCachedKeys (req);
final Dataset<Row> union AllcachedDf = getUnionAllHistoricalDfCache(req);
final Map<String, List<Document>> keyFilterMap = getCacheKeyFilterMap paramindicator. "VOLUME", req.getCacheDfRequest Param())
if (!ObjectUtils.allNotNull(cachedKeys, unionAllcachedDf)) {
isValid = false;
responseBuilder.requestState (SparkJobRequestState. REJECTED);
responseBuilder.detailMessage ("Unable to process request as DF cache is not initialized");
Logger.error("Unable to process request as DF cache is not initialized - {}", req);
}
final Sets.SetView<String> difference = Sets.difference (keyFilterMap.keySet(), cachedKeys);
if (isValid && !difference.isEmpty()) {
Logger.info("Missing required keys in DF cache. Spark Request-{} for input source-{}. "
+ "Cache DF keys required for this request - {}",
reg.getUniqueRequestId(), getInputSourceName (req), keyFilterMap.keySet());
Logger.info("Missing required keys in DF cache. Spark Request-{} for input collection-. Cached DF keys - {}",
reg.getUniqueRequestId(), getInputSourceName (req), cachedKeys);
}
if (getOrInitCachedSource (req).getLastCacheRefresh() == null
II getOrInitCachedSource (reg).getLastCache Refresh ().is Before (LocalDate.now().minusDays (1))) {
Logger.info("T-1 Delta DF refresh for {} is NOT completed yet. lastCacheRefresh-{"
+ "This report may have 1 day old data. RequestId- {}",
getInputSourceName (req), getOrInitCachedSource (req).getLastCacheRefresh(), req.getUniqueRequestId());
}
}
return isValid;}



private Dataset cache(final Dataset in) {
if (in != null) {
return in.persist (cacheStorageLevel);
}
public Dataset<Row> getCachedDF to ProcessRequest(final SparkJobRequest request) {
final Dataset<Row> df;
}
}
return null;
}
if (getUnionAllHistoricalDfCache (request) != null) {
(request);
df = getUnionHistoricalIntradayDfCache
} else {
df = null;
Logger.error("ERROR!!! Unable to get cached DF to process request Id-). Request [{}]",
request.getUniqueRequestId(), request);
return df;
public String getInputSourceName (final SparkJobRequest req) {
return dataSourceProvider.get (req, req.getInputDataSource().getDatasourceType()).getInputSourceName (req);
public void updateCacheRefreshDate (final SparkJobRequest request) {
final DfCacheData cacheData = getOrInitCachedSource (request);
if (cacheData.getLastCacheRefresh() == null
II request.getCacheDfRequest Param().getPeriod End ().isAfter (cacheData.getLastCacheRefresh())) {
cacheData.setLastCacheRefresh (request.getCacheDfRequest Param().getPeriodEnd());
this.logger.info("Updated Last DF cache refresh date-() for-{}",
cacheData.getLastCacheRefresh(),
getInputSourceName (request));

this.logger.info("Unable to update last DF cache refresh date-{} for-{}",
cacheData.getLastCache Refresh (), getInputSourceName (request));
}
} else {
public void overrideCacheRefreshDate (final SparkJobRequest request) {
final DfCacheData cache Data = getOrInitCachedSource (request);
cacheData.setLastCache Refresh (request.getCacheDfRequest Param().getPeriodEnd());



///////////////


public enum DfCacheSourceKey {
ENRICHED_TRADE (Enriched Trade.CACHE_NAME) {
@Override
public List<Document> getSparkPipelineFilter (final SparkJobRequest request,
final String paramIndicator) {
}
final CacheDfRequest Param cacheDfRequest Param = request.getCacheDfRequest Param();
return buildSparkPipelineFilterForGiven Period (
paramIndicator, cacheDfRequest Param.getPeriodStart(), cacheDfRequest Param.getPeriodEnd());
@Override
public Column [] getSparkColProjections (final SparkJobRequest request,
final Config config) {
return SparkDataSource.initColumn Projections
}
}
}

/////////////




@Data
@Getter
@Setter
@Builder (toBuilder = true)
@NoArgsConstructor
@ALLArgsConstructor
public class DfCacheData {
}
private DfCacheSourceKey id;
private final Set<String> cachedDfKeys = Sets.newConcurrentHashSet();
private Dataset<Row> union AllcachedDf = null;
private Dataset<Row> intraDaycachedDf = null;
private LocalDate lastCacheRefresh;
public void setUnionAllcachedDf(final Dataset<Row> unionAllcachedDf) {
if (this.unionAllcachedDf != null) {
}
this.unionAllcachedDf.unpersist();
this.unionAllcachedDf = unionAllcachedDf;
}
}
///////////////


public class MultiDimensionalAggFunction {
private final Config config;
private final Logger logger;
@Inject
public MultiDimensionalAggFunction (final LoggerFactory LoggerFactory, final Config config) {
this.config = config;
this.logger = LoggerFactory.create (MultiDimensionalAggFunction.class);
}
public Dataset<Row> aggregate (final SparkJobRequest request,
Dataset<Row> df,
final SparkJobResponse. SparkJobResponse Builder sparkRespBuilder) {
Dataset<Row> dfOutput = null;
final MultiDimensionalPivotParam pivotParam= request.getMultiDimensionalAggParam();
final List<SparkPivotParam> pivots = pivotParam.getPivots();
if (isNotEmpty (pivots)) {
dfOutput = df;
for (final SparkPivotParam p: pivots) {
final Column[] gpCols = sparkGroupCols (p);
final List<Column> aggFunctions = getSparkAggExpr (p.getAggregations (), logger);
if (isNotEmpty(aggFunctions) && !aggFunctions.is Empty() && StringUtils.isNotBlank (p.getPivotColName())) {
final Relational Grouped Dataset relationalDs = getRelational GroupedDataset (request, p. dfoutput, gpCols);
final Column first = aggFunctions.get(0);
if (aggFunctions.size() == 1) {
dfoutput = relationalDs,agg (first);
I
} else {
Column[] aggArray = new Column [aggFunctions.size()-1];
aggArray = aggFunctions, subList( : 1, aggFunctions.size()).toArray(aggArray);


}
}
dfOutput = relationalDs.agg (first, aggArray);
/**
}
} else {
sparkRespBuilder.requestState (SparkJobRequestState.FAILED);
Logger.warn("Request Id- {). Invalid SparkPivotParam:{}", request.getUniqueRequestId(), p);
return null;
dfOutput=dfoutput.na().fill(
value: OL);
dfOutput = dfOutput.na().fill( value: 8d);
dfOutput = renameColumns (pivotParam.getRenameColumns (), dfOutput);
dfOutput = applyBiRowAggregation Transformation (request, dfoutput, pivotParam.getSales AdditionalRowAggregation(), this.config);
dfoutput = additional AggColumns (pivotParam.getAdditionalAggColumns (), dfOutput);
dfOutput = calculateHmRatioColumns (request.getUniqueRequestId(), pivotParam.getHmRatioColumns (), dfOutput);
dfOutput = calculateBsPressureColumns (request.getUniqueRequestId(), pivotParam.getBsPressureColumns(), dfoutput);
dfOutput = addMandatoryColumnWithConstantValue (dfOutput, request);
} else {
sparkRespBuilder.requestState (SparkJobRequestState.FAILED);
I
Logger.warn("[SPARK: DEFINED_FUNCTION] Pivots not provided for Multi Dimensional aggregation. Request Id {}"
+ "Ignoring Job request :-} [ACTION: Please provide required parameters]", request.getUniqueRequestId(), request);
}
return dfOutput;
}


private Relational GroupedDataset getRelationalGroupedDataset (final SparkJobRequest request, final SparkPivotParam p, final Dataset<Row> dfOutput,
final Column[] gpCols) {
if (isNotEmpty(p.getPivotOutPutValues())) {
return dfoutput.groupBy (gpCols).pivot (col (p.getPivotColName()), p.getPivotOut PutValues());
} else {
final SparkBIRequestParams biRequest Params = request.getBiRequest Params ();
String splitBy = null;
if (biRequestParams != null) {
splitBy= biRequestParams.getParams () != null
? biRequestParams.getParams ().get("SplitBy")
= null;
}
if ("EcomVoice".equals(splitBy)) {
return dfoutput.withColumn (SOURCE, when (col(SOURCE).equalTo ("VOICE"), value: "VOICE")
.otherwise ("ECom"))
-groupBy (gpCols)
-pivot (col (p.getPivotColName()));
} else if ("AttemptedvsExecuted".equals(splitby)) {
final String[] statusExecutedValues= getStatus ExecutedValues (config);
return dfOutput.withColumn (STATUS, when (col(STATUS).isin (statusExecutedValues), value: "Executed")
.otherwise("Attempted"))
.groupBy (gpCols)
-pivot (col (p.getPivotColName()));
} else {
return dfoutput.groupBy (gpCols).pivot (col (p.getPivotColName()));
}
}
}




private static Column[] sparkGroupCols (final SparkPivotParam pivotParam) (
final int size = pivotParam.getGroupByCols ().size();
return pivotParam.getGroupByCols().stream().map (functions::col).collect(Collectors.toList()).toArray (new Column[size]);
}
private static Dataset<Row> renameColumns (Map<String, String> renameCols, Dataset<Row> inputDf) {
if (ObjectUtils.isNotEmpty (renameCols)) {
final List<String> existingDfCols = Arrays.asList (inputDf.columns());
for (final Map.Entry<String, String> e: renameCols.entrySet()) {
if (!existingDfCols.contains (e.getKey())) {
inputDf = inputDf.withColumn (e.getKey(), lit( literal: 0));
inputDf inputDf.withColumnRenamed (e.getKey(), e.getValue());
}
return inputDf;
private static Dataset<Row> applyBiRowAggregation Transformation (final SparkJobRequest request, Dataset<Row> inputDf, final List<String> addCols, final Config config){
if (ObjectUtils.isNotEmpty(addCols)) {
I
final List<String> existingDfCols = Arrays.asList (inputDf.columns());
Column firstCol = null;
final List<Column> aggRows = new ArrayList<> ();
for (final String column: addCols) {
if (existingDfCols.contains (column)) {
if (firstCol == null) {
firstcol = sum(col(column)).as (column);
aggRows.add(sum(col(column)).as (column));
} else {



if (firstCol == null) {
return inputDf;
}
aggRows.add(sum (col(column)).as (column));
final Column[] aggRowsArr = new Column [aggRows.size()];
aggRows.toArray(aggRows Arr);
final SparkBIRequest Params biRequest Params = request.getBiRequestParams ();
if ("AttVsEx".equals (biRequestParams.getAnalysis Type())) {
}
MisDashBoardSparkQuery.java
final String[] executedArr = getStatus ExecutedValues (config);
inputDf = inputDf.groupBy (when (col (STATUS).isin (executedArr), value: "Executed")
.otherwise("Attempted")
.as (STATUS))
agg (firstCol, aggRowsArr);
} else if ("VoiceVsECom".equals (biRequest Params.getAnalysis Type())) {
inputDf applyVoice VsEComTransformation (request, inputDf, first Col, aggRowsArr);
return inputDf;
MultiDimensional PivotParam.class x
SparkBIFunction.java
private static Dataset<Row> applyVoiceVsEComTransformation (final SparkJobRequest request, Dataset<Row> input f. final Column firstAggCol, final Column[] aggRowsArr){
final List<String> groupByStringColumns = request.getMultiDimensionalAggParam().getPivots().get().getGroupByCols():
final List<Column> groupByColumns = new ArrayList();






final Column eComVoiceGroupByColumn= when (col(SOURCE).isin ("VOICE"), value: "VOICE")
.otherwise ("ECom")
.as (SOURCE);
groupByColumns.add(eComVoiceGroupByColumn);
// get the remaining groupBy columns
if (groupByString Columns.size() > 1) {
groupByString Columns.stream()
}
.filter(col->SOURCE.equalsIgnoreCase (col))
.forEach(string Col groupByColumns.add(col (string Col)));
inputDf inputDf.groupBy (groupByColumns.toArray (new Column [0])).
.agg (firstAggCol, aggRowsArr);
return inputDf;
static Dataset<Row> additionalAggColumns (final Map<String, List<String>> addCols, Dataset<Row> inputDf) {
if (ObjectUtils.isNotEmpty (addCols)) {
final Set<String> existingDfCols = new HashSet<> (Arrays.asList (inputDf.columns()));
for (final Map.Entry<String, List<String>> e: addCols.entrySet()) {
if (CollectionUtils.isNotEmpty(e.getValue())) {
Column aggColl = null;
for (final String coll: e.getValue()) {
if (existingDfCols.contains (coll)) {
if (aggColl == null) {
aggColl col(coll);
MisDashBoardSparkQuery.java x MultiDimensional PivotParam.clas
} else {
}
aggColl = aggColl.plus (col (coll));}




}
}
if (aggColl != null) {
inputDf = inputDf.withColumn (e.getKey(), aggColl);
} else {
inputDf inputDf.withColumn (e.getKey(), lit(literal: 0));
}
existingDfCols.add(e.getKey());
} else {
inputDf inputDf.withColumn (e.getKey(), lit (literal: 0));
}
return inputDf;
ANTIR Preview
private Dataset<Row> calculateHmRatioColumns (String requestId,
if (ObjectUtils.isNotEmpty (hmRatioColumns)) {
for (Map.Entry<String, Pair<String, String>>e: hmRatioColumns.entrySet()) {
if (StringUtils.isNotEmpty(e.getKey()) && Objects.nonNull(e.getValue())) {
String numerator = e.getValue().getLeft();
String denominator = e.getValue().getRight();
Map<String, Pair<String, String>> hmRatio Columns, Dataset<Row> inputDf) {
} else {
4
inputDf = inputDf.withColumn (e.getKey(),
when(col(denominator).gt (lit( literal: 0)), col (numerator). divide (col (denominator))).otherwise (lit( literal: e)));
Logger.warn("Request Id- {}, key (), Invalid param for HmRatio calculation.", requestId, e.getKey());
}




public Dataset<Row> addMandatoryColumnWithConstantValue (Dataset<Row> sparkDs, SparkJobRequest req) {
Map<Pair<String, String>, Object> mandatoryCol = req.getMandatoryDfColumns();
if (MapUtils.isNotEmpty (mandatoryCol)) {
}
List<String> existingDfCols = Arrays.asList(sparkDs.columns());
for (Map.Entry<Pair<String, String>, Object> e mandatoryCol.entrySet()) {
}
if (e.getKey() != null) {
if (!existingDfCols.contains (e.getKey().getLeft())) {
sparkDs = sparkDs.withColumn (e.getKey().getLeft(), lit(e.getValue()));
}
sparkDs = sparkDs.withColumnRenamed (e.getKey().getLeft(), e.getKey().getRight());
sparkDs = sparkDs.withColumn (e.getKey().getRight(), when (col(e.getKey().getRight()).isNullO), Lit(e.getValue(
.otherwise (col (e.getKey().getRight())));
}
return sparkDs;
private static String[] getStatusExecutedValues (final Config config) {
final String executedStatus = SparkCoreConfigDefinitions.SPARK_CORE_STATUS_EXECUTED_VALUES.get(config);
return executedStatus.split( regex: ",");

