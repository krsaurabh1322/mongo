
public class JoinConfig {
    private String leftDataset;
    private String rightDataset;
    private List<String> joinColumns;
    private JoinType joinType;
    private List<String> selectColumnsRight;
    private List<String> groupByColumnsRight;
    private Map<String, String> aggregateFunctionsRight;
    private String filterRight;
    private String havingClauseRight; // New attribute for HAVING clause

    // Constructors, getters, setters, etc.
}


@Override
public Dataset<Row> load(SparkJobRequest request, JoinConfig joinConfig) {
    Dataset<Row> leftDataset = loadLeftDataset(request);
    Dataset<Row> rightDataset = loadRightDatasetForJoin(request, joinConfig);

    // Perform the join
    Dataset<Row> result = leftDataset.join(rightDataset, joinConfig.getJoinColumns(), joinConfig.getJoinType().toString());

    // Apply filtering
    if (joinConfig.getFilterRight() != null) {
        result = result.filter(joinConfig.getFilterRight());
    }

    // Apply group by and aggregation
    if (!joinConfig.getGroupByColumnsRight().isEmpty() && !joinConfig.getAggregateFunctionsRight().isEmpty()) {
        result = applyGroupByAndAggregation(result, joinConfig.getGroupByColumnsRight(), joinConfig.getAggregateFunctionsRight());

        // Apply HAVING clause
        if (joinConfig.getHavingClauseRight() != null) {
            result = result.having(joinConfig.getHavingClauseRight());
        }
    }

    // Apply select columns
    if (!joinConfig.getSelectColumnsRight().isEmpty()) {
        result = applySelectColumns(result, joinConfig.getSelectColumnsRight());
    }

    return result;
}

private Dataset<Row> applyGroupByAndAggregation(Dataset<Row> inputDF, List<String> groupByColumns, Map<String, String> aggregateFunctions) {
    Column[] groupByExpr = groupByColumns.stream().map(functions::col).toArray(Column[]::new);
    List<Column> aggExprList = new ArrayList<>();

    for (Map.Entry<String, String> entry : aggregateFunctions.entrySet()) {
        String columnName = entry.getKey();
        String aggFunction = entry.getValue();
        aggExprList.add(functions.callUDF(aggFunction, functions.col(columnName)).as(columnName));
    }

    return inputDF.groupBy(groupByExpr).agg(aggExprList.toArray(new Column[0]));
}

private Dataset<Row> applySelectColumns(Dataset<Row> inputDF, List<String> selectColumns) {
    Column[] selectExpr = selectColumns.stream().map(functions::col).toArray(Column[]::new);
    return inputDF.select(selectExpr);
}


private Dataset<Row> loadRightDatasetForJoin(SparkJobRequest request, JoinConfig joinConfig) {
    SparkDataSourceConfig rightDataSourceConfig = joinConfig.getRightDatasetConfig();
    SparkDataSource rightDataSource = dataSourceProvider.get(request, rightDataSourceConfig.getDatasourceType());

    if (rightDataSource != null) {
        Dataset<Row> rightDF = rightDataSource.load(request, rightDataSourceConfig);
        // Apply filtering if needed
        if (joinConfig.getFilterRight() != null) {
            rightDF = rightDF.filter(functions.expr(joinConfig.getFilterRight()));
        }
        return rightDF;
    } else {
        // Handle error or return an empty DataFrame
        return sparkSession.emptyDataFrame();
    }
}


public class SparkMongoDataSource implements SparkDataSource<List<Document>> {

    // ... (existing code)

    @Override
    public Dataset<Row> Load(final SparkJobRequest request, final List<Document> filter) {
        // Load the base dataset
        final SparkDataSourceConfig inputDataSourceConfig = request.getInputDataSource();
        final SparkDataSource inputDataSource = dataSourceProvider.get(request, inputDataSourceConfig.getDatasourceType());

        if (inputDataSource == null) {
            // Handle error or return an empty DataFrame
            return sparkSession.emptyDataFrame();
        }

        Dataset<Row> baseDF = inputDataSource.load(request, filter);

        // Perform joins with JoinConfig objects
        for (JoinConfig joinConfig : request.getJoinConfigs()) {
            Dataset<Row> rightDF = loadRightDatasetForJoin(request, joinConfig);

            if (rightDF != null) {
                baseDF = performJoin(baseDF, rightDF, joinConfig);
            }
        }

        return baseDF;
    }

    private Dataset<Row> loadRightDatasetForJoin(SparkJobRequest request, JoinConfig joinConfig) {
        // ... (previous implementation)
    }

    private Dataset<Row> performJoin(Dataset<Row> leftDF, Dataset<Row> rightDF, JoinConfig joinConfig) {
        String joinType = joinConfig.getJoinType().toString();
        String[] joinColumns = joinConfig.getJoinColumns().toArray(new String[0]);
        
        // Perform the join operation
        Dataset<Row> joinedDF = leftDF.join(rightDF, joinColumns, joinType);
        
        // Apply HAVING clause if specified
        if (joinConfig.getHavingClauseRight() != null) {
            joinedDF = joinedDF.filter(functions.expr(joinConfig.getHavingClauseRight()));
        }
        
        // Apply group by and aggregation
        if (!joinConfig.getGroupByColumnsRight().isEmpty() || !joinConfig.getAggregateFunctionsRight().isEmpty()) {
            joinedDF = applyGroupByAndAggregation(joinedDF, joinConfig);
        }
        
        // Apply select columns
        if (!joinConfig.getSelectColumnsRight().isEmpty()) {
            joinedDF = applySelectColumns(joinedDF, joinConfig.getSelectColumnsRight());
        }
        
        // Apply filters
        if (joinConfig.getFilterRight() != null) {
            joinedDF = joinedDF.filter(functions.expr(joinConfig.getFilterRight()));
        }
        
        return joinedDF;
    }

    // ... (other methods)
}

//// Test class////
import org.apache.spark.sql.*;
import org.junit.*;

import java.util.*;

public class SparkMongoDataSourceTest {

    private SparkSession spark;

    @Before
    public void setup() {
        spark = SparkSession.builder()
                .appName("SparkMongoDataSourceTest")
                .master("local[*]")
                .getOrCreate();
    }

    @After
    public void cleanup() {
        spark.stop();
    }

    @Test
    public void testJoinWithConditions() {
        // Create sample data for base dataset
        List<Row> baseData = Arrays.asList(
                RowFactory.create("user1", 100),
                RowFactory.create("user2", 200)
        );

        // Create sample data for right datasets
        List<Row> rightData1 = Arrays.asList(
                RowFactory.create("user1", 50),
                RowFactory.create("user2", 75)
        );

        List<Row> rightData2 = Arrays.asList(
                RowFactory.create("user1", "A"),
                RowFactory.create("user2", "B")
        );

        // Create schemas
        StructType baseSchema = new StructType()
                .add("userId", DataTypes.StringType)
                .add("amount", DataTypes.IntegerType);

        StructType rightSchema1 = new StructType()
                .add("userId", DataTypes.StringType)
                .add("bonus", DataTypes.IntegerType);

        StructType rightSchema2 = new StructType()
                .add("userId", DataTypes.StringType)
                .add("category", DataTypes.StringType);

        // Create DataFrames
        Dataset<Row> baseDF = spark.createDataFrame(baseData, baseSchema);
        Dataset<Row> rightDF1 = spark.createDataFrame(rightData1, rightSchema1);
        Dataset<Row> rightDF2 = spark.createDataFrame(rightData2, rightSchema2);

        // Create SparkJobRequest
        SparkJobRequest request = new SparkJobRequest();
        request.setInputDataSource(new SparkDataSourceConfig());  // Set appropriate config
        request.addJoinConfig(createJoinConfig1());
        request.addJoinConfig(createJoinConfig2());

        // Create SparkMongoDataSource
        SparkMongoDataSource dataSource = new SparkMongoDataSource(/* pass necessary dependencies */);

        // Load the final joined dataset
        Dataset<Row> result = dataSource.Load(request, null);

        // Show the result
        result.show();
    }

    private JoinConfig createJoinConfig1() {
        JoinConfig config = new JoinConfig(JoinConfig.JoinType.INNER);
        config.setLeftDataset("base");
        config.setRightDataset("right1");
        config.addJoinCondition("userId", "userId", null);
        config.addSelectColumn("userId");
        config.addSelectColumn("amount");
        config.addSelectColumn("bonus");
        config.addAggregateFunction("amount", "sum");
        config.addAggregateFunction("bonus", "min");
        config.addGroupByColumn("userId");
        config.setFilterRight("bonus > 50");
        return config;
    }

    private JoinConfig createJoinConfig2() {
        JoinConfig config = new JoinConfig(JoinConfig.JoinType.INNER);
        config.setLeftDataset("result1");
        config.setRightDataset("right2");
        config.addJoinCondition("userId", "userId", null);
        config.addSelectColumn("userId");
        config.addSelectColumn("amount_sum");
        config.addSelectColumn("bonus_min");
        config.addSelectColumn("category");
        config.addAggregateFunction("amount_sum", "sum");
        config.addAggregateFunction("bonus_min", "min");
        config.addGroupByColumn("userId");
        config.addGroupByColumn("category");
        config.addHavingClauseRight("sum(amount_sum) > 200");
        return config;
    }
}

///////
correction



private Dataset<Row> applyGroupByAndAggregation(Dataset<Row> input, JoinConfig joinConfig) {
    // Group by the specified columns
    List<Column> groupByCols = joinConfig.getGroupByColumnsRight().stream()
            .map(input::col)
            .collect(Collectors.toList());

    // Prepare aggregation expressions
    List<Column> aggExprs = new ArrayList<>();
    for (Map.Entry<String, String> entry : joinConfig.getAggregateFunctionsRight().entrySet()) {
        String colName = entry.getKey();
        String aggFunc = entry.getValue();
        Column aggExpr = functions.expr(aggFunc + "(" + colName + ")").as(colName + "_" + aggFunc);
        aggExprs.add(aggExpr);
    }

    // Apply aggregation using agg() method
    Dataset<Row> result = input.groupBy(groupByCols.toArray(new Column[0]))
            .agg(aggExprs.get(0), JavaConverters.asScalaBuffer(aggExprs.subList(1, aggExprs.size())).toSeq());

    return result;
}



///////////
Mongo DB


here are the MongoDB commands to insert data into the respective collections:

1. **Insert Data into 'sales' Collection:**

```javascript
db.sales.insertOne({
  _id: 1,
  customer_id: 101,
  product_id: 201,
  amount: 500,
  bonus: 50,
  category: "A"
})
```

2. **Insert Data into 'customers' Collection:**

```javascript
db.customers.insertOne({
  _id: 101,
  name: "John Doe",
  age: 30
})
```

3. **Insert Data into 'products' Collection:**

```javascript
db.products.insertOne({
  _id: 201,
  name: "Product A",
  price: 100,
  category: "A"
})
```

Replace the data and values as needed to match your test cases and schema.


//////////////



Sure, let's assume the `sales`, `bonus`, and `category` collections are inserted as follows:

1. Insert data into the `sales` collection:
```javascript
db.sales.insertMany([
  { customerId: 1, productId: 101, amount: 100 },
  { customerId: 1, productId: 102, amount: 150 },
  { customerId: 2, productId: 101, amount: 200 },
  { customerId: 3, productId: 103, amount: 50 }
]);
```

2. Insert data into the `bonus` collection:
```javascript
db.bonus.insertMany([
  { customerId: 1, bonusAmount: 10 },
  { customerId: 2, bonusAmount: 15 },
  { customerId: 4, bonusAmount: 5 }
]);
```

3. Insert data into the `category` collection:
```javascript
db.category.insertMany([
  { productId: 101, category: "Electronics" },
  { productId: 102, category: "Clothing" },
  { productId: 103, category: "Books" }
]);
```

The SQL join query that we are testing here corresponds to the following logic:

```sql
SELECT
  s.customerId,
  c.category,
  SUM(s.amount) AS totalAmount,
  SUM(b.bonusAmount) AS totalBonus
FROM
  sales s
INNER JOIN
  bonus b ON s.customerId = b.customerId
INNER JOIN
  category c ON s.productId = c.productId
GROUP BY
  s.customerId, c.category;
```

This query joins the `sales`, `bonus`, and `category` collections based on specified conditions and then performs grouping and aggregation operations to calculate the total amount and total bonus for each customer in each category.



public static JoinConfig createJoinConfig1() {
    List<String> joinColumns = new ArrayList<>();
    joinColumns.add("customerId");

    List<String> selectColumnsRight = new ArrayList<>();
    selectColumnsRight.add("customerId");
    selectColumnsRight.add("amount");
    selectColumnsRight.add("bonusAmount");

    List<String> groupByColumnsRight = new ArrayList<>();
    groupByColumnsRight.add("customerId");

    Map<String, String> aggregateFunctionsRight = new HashMap<>();
    aggregateFunctionsRight.put("amount", "sum");
    aggregateFunctionsRight.put("bonusAmount", "sum");

    return new JoinConfig("sales", "bonus", joinColumns, JoinType.INNER,
        selectColumnsRight, groupByColumnsRight, aggregateFunctionsRight, null);
}

public static JoinConfig createJoinConfig2() {
    List<String> joinColumns = new ArrayList<>();
    joinColumns.add("productId");

    List<String> selectColumnsRight = new ArrayList<>();
    selectColumnsRight.add("productId");
    selectColumnsRight.add("category");

    List<String> groupByColumnsRight = new ArrayList<>();
    groupByColumnsRight.add("productId");

    Map<String, String> aggregateFunctionsRight = new HashMap<>();
    aggregateFunctionsRight.put("category", "first");

    return new JoinConfig("sales_bonus", "category", joinColumns, JoinType.INNER,
        selectColumnsRight, groupByColumnsRight, aggregateFunctionsRight, null);
}



/////final/////

private Dataset<Row> performJoins(
    SparkSession sparkSession,
    SparkJobRequest request,
    JoinConfig joinConfig,
    Dataset<Row> baseDataset
) {
    Dataset<Row> result = baseDataset;

    for (JoinConfig.JoinCondition joinCondition : joinConfig.getJoinConditions()) {
        String[] joinCols = getJoinColumns(joinCondition, result, joinConfig);
        Dataset<Row> rightDataset = loadRightDatasetForJoin(request, joinConfig);
        
        String filterConditions = getFilterConditions(joinCondition, rightDataset, joinConfig);

        result = result
            .join(rightDataset, joinCols, joinType)
            .filter(filterConditions)
            .groupBy(getGroupByColumns(joinConfig))
            .agg(getAggregationExpressions(joinConfig));

        applySelectColumns(result, joinConfig.getSelectColumnsRight());
    }

    return result;
}


import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import static org.apache.spark.sql.functions.*;

public class SparkMongoDataSource {
    // ... your existing code ...

    private String[] getJoinColumns(
        JoinConfig.JoinCondition joinCondition,
        Dataset<Row> leftDataset,
        JoinConfig joinConfig
    ) {
        String leftCol = joinCondition.getLeftColumn();
        String rightCol = joinCondition.getRightColumn();

        String leftDatasetAlias = getAliasForDataset(leftDataset, joinConfig);
        String rightDatasetAlias = getAliasForDataset(loadRightDatasetForJoin(request, joinConfig), joinConfig);

        String[] joinCols = new String[]{
            leftDatasetAlias + "." + leftCol,
            rightDatasetAlias + "." + rightCol
        };

        return joinCols;
    }

    private String getFilterConditions(
        JoinConfig.JoinCondition joinCondition,
        Dataset<Row> rightDataset,
        JoinConfig joinConfig
    ) {
        // Combine filter conditions from both joinConfig and joinCondition
        String joinFilter = joinCondition.getFilter();
        String rightFilter = joinConfig.getFilterRight();

        if (joinFilter != null && !joinFilter.isEmpty()) {
            joinFilter = "(" + joinFilter + ")";
        }

        if (rightFilter != null && !rightFilter.isEmpty()) {
            if (joinFilter != null && !joinFilter.isEmpty()) {
                joinFilter = joinFilter + " AND ";
            }
            joinFilter = "(" + rightFilter + ")";
        }

        String rightDatasetAlias = getAliasForDataset(rightDataset, joinConfig);

        return joinFilter.replaceAll("right\\.", rightDatasetAlias + ".");
    }

    private String getAliasForDataset(Dataset<Row> dataset, JoinConfig joinConfig) {
        // Generate a unique alias for the dataset
        return "ds_" + dataset.hashCode();
    }

    private String[] getGroupByColumns(JoinConfig joinConfig) {
        return joinConfig.getGroupByColumns().toArray(new String[0]);
    }

    private Column[] getAggregationExpressions(JoinConfig joinConfig) {
        return joinConfig.getAggregateFunctions().entrySet().stream()
            .map(entry -> {
                String columnName = entry.getKey();
                String aggregateFunction = entry.getValue();
                return expr(aggregateFunction + "(" + columnName + ") as " + columnName);
            })
            .toArray(Column[]::new);
    }
}



import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class JoinConfig {
    public enum JoinType {
        INNER, LEFT, RIGHT, FULL
    }

    private JoinType joinType;
    private List<JoinCondition> joinConditions;
    private List<String> groupByColumns;
    private List<String> selectColumns;
    private Map<String, String> aggregateFunctions;

    // New fields for filters on right dataset
    private String filterRight;

    public JoinConfig(JoinType joinType) {
        this.joinType = joinType;
        this.joinConditions = new ArrayList<>();
        this.groupByColumns = new ArrayList<>();
        this.selectColumns = new ArrayList<>();
        this.aggregateFunctions = new HashMap<>();
    }

    public void addJoinCondition(String leftColumn, String rightColumn, String filter) {
        joinConditions.add(new JoinCondition(leftColumn, rightColumn, filter));
    }

    // Other methods...

    public String getFilterRight() {
        return filterRight;
    }

    public void setFilterRight(String filterRight) {
        this.filterRight = filterRight;
    }

    public static class JoinCondition {
        private String leftColumn;
        private String rightColumn;
        private String filter;

        public JoinCondition(String leftColumn, String rightColumn, String filter) {
            this.leftColumn = leftColumn;
            this.rightColumn = rightColumn;
            this.filter = filter;
        }

        public String getLeftColumn() {
            return leftColumn;
        }

        public String getRightColumn() {
            return rightColumn;
        }

        public String getFilter() {
            return filter;
        }
    }
}


public class SparkMongoDataSource {
    // ... other methods ...

    private Column getFilterCondition(
        JoinConfig.JoinCondition joinCondition,
        Dataset<Row> rightDataset,
        JoinConfig joinConfig
    ) {
        String joinFilter = joinCondition.getFilter();
        String rightFilter = joinConfig.getFilterRight();

        if (joinFilter != null && !joinFilter.isEmpty()) {
            joinFilter = "(" + joinFilter + ")";
        }

        if (rightFilter != null && !rightFilter.isEmpty()) {
            if (joinFilter != null && !joinFilter.isEmpty()) {
                joinFilter = joinFilter + " AND ";
            }
            joinFilter = "(" + rightFilter + ")";
        }

        return expr(joinFilter);
    }

    private String getAliasForDataset(Dataset<Row> dataset, JoinConfig joinConfig) {
        // ... unchanged ...
    }
}




import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class JoinConfig {
    public enum JoinType {
        INNER, LEFT, RIGHT, FULL
    }

    private JoinType joinType;
    private List<JoinCondition> joinConditions;
    private List<String> groupByColumns;
    private List<String> selectColumns;
    private Map<String, String> aggregateFunctions;

    // New fields for filters on right dataset
    private String filterRight;

    public JoinConfig(JoinType joinType) {
        this.joinType = joinType;
        this.joinConditions = new ArrayList<>();
        this.groupByColumns = new ArrayList<>();
        this.selectColumns = new ArrayList<>();
        this.aggregateFunctions = new HashMap<>();
    }

    public void addJoinCondition(String leftColumn, String rightColumn, String filter) {
        joinConditions.add(new JoinCondition(leftColumn, rightColumn, filter));
    }

    public JoinType getJoinType() {
        return joinType;
    }

    public List<JoinCondition> getJoinConditions() {
        return joinConditions;
    }

    public List<String> getGroupByColumns() {
        return groupByColumns;
    }

    public List<String> getSelectColumns() {
        return selectColumns;
    }

    public Map<String, String> getAggregateFunctions() {
        return aggregateFunctions;
    }

    public String getFilterRight() {
        return filterRight;
    }

    public void setFilterRight(String filterRight) {
        this.filterRight = filterRight;
    }

    public static class JoinCondition {
        private String leftColumn;
        private String rightColumn;
        private String filter;

        public JoinCondition(String leftColumn, String rightColumn, String filter) {
            this.leftColumn = leftColumn;
            this.rightColumn = rightColumn;
            this.filter = filter;
        }

        public String getLeftColumn() {
            return leftColumn;
        }

        public String getRightColumn() {
            return rightColumn;
        }

        public String getFilter() {
            return filter;
        }
    }
}



