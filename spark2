
public class JoinConfig {
    private String leftDataset;
    private String rightDataset;
    private List<String> joinColumns;
    private JoinType joinType;
    private List<String> selectColumnsRight;
    private List<String> groupByColumnsRight;
    private Map<String, String> aggregateFunctionsRight;
    private String filterRight;
    private String havingClauseRight; // New attribute for HAVING clause

    // Constructors, getters, setters, etc.
}


@Override
public Dataset<Row> load(SparkJobRequest request, JoinConfig joinConfig) {
    Dataset<Row> leftDataset = loadLeftDataset(request);
    Dataset<Row> rightDataset = loadRightDatasetForJoin(request, joinConfig);

    // Perform the join
    Dataset<Row> result = leftDataset.join(rightDataset, joinConfig.getJoinColumns(), joinConfig.getJoinType().toString());

    // Apply filtering
    if (joinConfig.getFilterRight() != null) {
        result = result.filter(joinConfig.getFilterRight());
    }

    // Apply group by and aggregation
    if (!joinConfig.getGroupByColumnsRight().isEmpty() && !joinConfig.getAggregateFunctionsRight().isEmpty()) {
        result = applyGroupByAndAggregation(result, joinConfig.getGroupByColumnsRight(), joinConfig.getAggregateFunctionsRight());

        // Apply HAVING clause
        if (joinConfig.getHavingClauseRight() != null) {
            result = result.having(joinConfig.getHavingClauseRight());
        }
    }

    // Apply select columns
    if (!joinConfig.getSelectColumnsRight().isEmpty()) {
        result = applySelectColumns(result, joinConfig.getSelectColumnsRight());
    }

    return result;
}

private Dataset<Row> applyGroupByAndAggregation(Dataset<Row> inputDF, List<String> groupByColumns, Map<String, String> aggregateFunctions) {
    Column[] groupByExpr = groupByColumns.stream().map(functions::col).toArray(Column[]::new);
    List<Column> aggExprList = new ArrayList<>();

    for (Map.Entry<String, String> entry : aggregateFunctions.entrySet()) {
        String columnName = entry.getKey();
        String aggFunction = entry.getValue();
        aggExprList.add(functions.callUDF(aggFunction, functions.col(columnName)).as(columnName));
    }

    return inputDF.groupBy(groupByExpr).agg(aggExprList.toArray(new Column[0]));
}

private Dataset<Row> applySelectColumns(Dataset<Row> inputDF, List<String> selectColumns) {
    Column[] selectExpr = selectColumns.stream().map(functions::col).toArray(Column[]::new);
    return inputDF.select(selectExpr);
}


private Dataset<Row> loadRightDatasetForJoin(SparkJobRequest request, JoinConfig joinConfig) {
    SparkDataSourceConfig rightDataSourceConfig = joinConfig.getRightDatasetConfig();
    SparkDataSource rightDataSource = dataSourceProvider.get(request, rightDataSourceConfig.getDatasourceType());

    if (rightDataSource != null) {
        Dataset<Row> rightDF = rightDataSource.load(request, rightDataSourceConfig);
        // Apply filtering if needed
        if (joinConfig.getFilterRight() != null) {
            rightDF = rightDF.filter(functions.expr(joinConfig.getFilterRight()));
        }
        return rightDF;
    } else {
        // Handle error or return an empty DataFrame
        return sparkSession.emptyDataFrame();
    }
}


public class SparkMongoDataSource implements SparkDataSource<List<Document>> {

    // ... (existing code)

    @Override
    public Dataset<Row> Load(final SparkJobRequest request, final List<Document> filter) {
        // Load the base dataset
        final SparkDataSourceConfig inputDataSourceConfig = request.getInputDataSource();
        final SparkDataSource inputDataSource = dataSourceProvider.get(request, inputDataSourceConfig.getDatasourceType());

        if (inputDataSource == null) {
            // Handle error or return an empty DataFrame
            return sparkSession.emptyDataFrame();
        }

        Dataset<Row> baseDF = inputDataSource.load(request, filter);

        // Perform joins with JoinConfig objects
        for (JoinConfig joinConfig : request.getJoinConfigs()) {
            Dataset<Row> rightDF = loadRightDatasetForJoin(request, joinConfig);

            if (rightDF != null) {
                baseDF = performJoin(baseDF, rightDF, joinConfig);
            }
        }

        return baseDF;
    }

    private Dataset<Row> loadRightDatasetForJoin(SparkJobRequest request, JoinConfig joinConfig) {
        // ... (previous implementation)
    }

    private Dataset<Row> performJoin(Dataset<Row> leftDF, Dataset<Row> rightDF, JoinConfig joinConfig) {
        String joinType = joinConfig.getJoinType().toString();
        String[] joinColumns = joinConfig.getJoinColumns().toArray(new String[0]);
        
        // Perform the join operation
        Dataset<Row> joinedDF = leftDF.join(rightDF, joinColumns, joinType);
        
        // Apply HAVING clause if specified
        if (joinConfig.getHavingClauseRight() != null) {
            joinedDF = joinedDF.filter(functions.expr(joinConfig.getHavingClauseRight()));
        }
        
        // Apply group by and aggregation
        if (!joinConfig.getGroupByColumnsRight().isEmpty() || !joinConfig.getAggregateFunctionsRight().isEmpty()) {
            joinedDF = applyGroupByAndAggregation(joinedDF, joinConfig);
        }
        
        // Apply select columns
        if (!joinConfig.getSelectColumnsRight().isEmpty()) {
            joinedDF = applySelectColumns(joinedDF, joinConfig.getSelectColumnsRight());
        }
        
        // Apply filters
        if (joinConfig.getFilterRight() != null) {
            joinedDF = joinedDF.filter(functions.expr(joinConfig.getFilterRight()));
        }
        
        return joinedDF;
    }

    // ... (other methods)
}

//// Test class////
import org.apache.spark.sql.*;
import org.junit.*;

import java.util.*;

public class SparkMongoDataSourceTest {

    private SparkSession spark;

    @Before
    public void setup() {
        spark = SparkSession.builder()
                .appName("SparkMongoDataSourceTest")
                .master("local[*]")
                .getOrCreate();
    }

    @After
    public void cleanup() {
        spark.stop();
    }

    @Test
    public void testJoinWithConditions() {
        // Create sample data for base dataset
        List<Row> baseData = Arrays.asList(
                RowFactory.create("user1", 100),
                RowFactory.create("user2", 200)
        );

        // Create sample data for right datasets
        List<Row> rightData1 = Arrays.asList(
                RowFactory.create("user1", 50),
                RowFactory.create("user2", 75)
        );

        List<Row> rightData2 = Arrays.asList(
                RowFactory.create("user1", "A"),
                RowFactory.create("user2", "B")
        );

        // Create schemas
        StructType baseSchema = new StructType()
                .add("userId", DataTypes.StringType)
                .add("amount", DataTypes.IntegerType);

        StructType rightSchema1 = new StructType()
                .add("userId", DataTypes.StringType)
                .add("bonus", DataTypes.IntegerType);

        StructType rightSchema2 = new StructType()
                .add("userId", DataTypes.StringType)
                .add("category", DataTypes.StringType);

        // Create DataFrames
        Dataset<Row> baseDF = spark.createDataFrame(baseData, baseSchema);
        Dataset<Row> rightDF1 = spark.createDataFrame(rightData1, rightSchema1);
        Dataset<Row> rightDF2 = spark.createDataFrame(rightData2, rightSchema2);

        // Create SparkJobRequest
        SparkJobRequest request = new SparkJobRequest();
        request.setInputDataSource(new SparkDataSourceConfig());  // Set appropriate config
        request.addJoinConfig(createJoinConfig1());
        request.addJoinConfig(createJoinConfig2());

        // Create SparkMongoDataSource
        SparkMongoDataSource dataSource = new SparkMongoDataSource(/* pass necessary dependencies */);

        // Load the final joined dataset
        Dataset<Row> result = dataSource.Load(request, null);

        // Show the result
        result.show();
    }

    private JoinConfig createJoinConfig1() {
        JoinConfig config = new JoinConfig(JoinConfig.JoinType.INNER);
        config.setLeftDataset("base");
        config.setRightDataset("right1");
        config.addJoinCondition("userId", "userId", null);
        config.addSelectColumn("userId");
        config.addSelectColumn("amount");
        config.addSelectColumn("bonus");
        config.addAggregateFunction("amount", "sum");
        config.addAggregateFunction("bonus", "min");
        config.addGroupByColumn("userId");
        config.setFilterRight("bonus > 50");
        return config;
    }

    private JoinConfig createJoinConfig2() {
        JoinConfig config = new JoinConfig(JoinConfig.JoinType.INNER);
        config.setLeftDataset("result1");
        config.setRightDataset("right2");
        config.addJoinCondition("userId", "userId", null);
        config.addSelectColumn("userId");
        config.addSelectColumn("amount_sum");
        config.addSelectColumn("bonus_min");
        config.addSelectColumn("category");
        config.addAggregateFunction("amount_sum", "sum");
        config.addAggregateFunction("bonus_min", "min");
        config.addGroupByColumn("userId");
        config.addGroupByColumn("category");
        config.addHavingClauseRight("sum(amount_sum) > 200");
        return config;
    }
}

///////
correction



private Dataset<Row> applyGroupByAndAggregation(Dataset<Row> input, JoinConfig joinConfig) {
    // Group by the specified columns
    List<Column> groupByCols = joinConfig.getGroupByColumnsRight().stream()
            .map(input::col)
            .collect(Collectors.toList());

    // Prepare aggregation expressions
    List<Column> aggExprs = new ArrayList<>();
    for (Map.Entry<String, String> entry : joinConfig.getAggregateFunctionsRight().entrySet()) {
        String colName = entry.getKey();
        String aggFunc = entry.getValue();
        Column aggExpr = functions.expr(aggFunc + "(" + colName + ")").as(colName + "_" + aggFunc);
        aggExprs.add(aggExpr);
    }

    // Apply aggregation using agg() method
    Dataset<Row> result = input.groupBy(groupByCols.toArray(new Column[0]))
            .agg(aggExprs.get(0), JavaConverters.asScalaBuffer(aggExprs.subList(1, aggExprs.size())).toSeq());

    return result;
}

